{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cb12017-4f46-4953-8dac-a51ca825f491",
   "metadata": {},
   "source": [
    "# 1.Creating the model and training it with Simple Intents "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dca14ad-bf15-40d9-beda-27d045332c30",
   "metadata": {},
   "source": [
    "This code is for training a chatbot model using TensorFlow and Natural Language Processing (NLP) techniques.\r\n",
    "Here's a breakdown of the code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74151038-c7ba-4830-8a8b-5d34c1539b15",
   "metadata": {},
   "source": [
    "#### 1.Imports:\n",
    "This section imports the necessary libraries for data manipulation (json, pickle, numpy), machine learning (tensorflow), text processing (wikipedia, nltk), and defines a WordNetLemmatizer for word normalization. It then loads the chatbot intents from a JSON file, which defines conversation patterns and corresponding responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a74e840-5d26-4399-be6e-1a00dc03ae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import wikipedia\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Initialize lemmatizer for word normalization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load chatbot intents from JSON file (conversation patterns and responses)\n",
    "intents = json.loads(open('D:\\Python Projects\\ChatBot\\Intent.json').read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68e1927-27d9-4f09-b419-306149b65bd7",
   "metadata": {},
   "source": [
    "### 2.Data Preprocessing\n",
    "Loads chatbot intents from a JSON file (defines conversation patterns and responses).\r\n",
    "Extracts words from conversation patterns and removes punctuation.\r\n",
    "Lemmatizes words (reduces words to their base form, e.g., \"running\" -> \"run\").\r\n",
    "Creates a vocabulary of unique words and a list of intent classes.\r\n",
    "Saves the vocabulary and classes as pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942ec89d-92a7-45e1-b9e1-fa212e71fea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []  # List to store all unique words from conversation patterns\n",
    "classes = []  # List to store all unique intent classes\n",
    "documents = []  # List to store (pattern, intent) pairs\n",
    "ignoreLetters = ['?', '!', '.', ',']  # List of punctuation to remove\n",
    "\n",
    "# Extract words, remove punctuation, and lemmatize from each conversation pattern\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['text']:\n",
    "        wordList = nltk.word_tokenize(pattern)  # Tokenize the pattern into words\n",
    "        words.extend(wordList)  # Add words to the words list\n",
    "        documents.append((wordList, intent['intent']))  # Add (pattern, intent) pair\n",
    "        if intent['intent'] not in classes:  # Add unique intent class to classes list\n",
    "            classes.append(intent['intent'])\n",
    "\n",
    "# Remove punctuation and lemmatize all words in the vocabulary\n",
    "words = [lemmatizer.lemmatize(word) for word in words if word not in ignoreLetters]\n",
    "# Sort and remove duplicates to create a unique vocabulary\n",
    "words = sorted(set(words))\n",
    "\n",
    "# Sort the list of intent classes\n",
    "classes = sorted(set(classes))\n",
    "\n",
    "# Save the vocabulary and classes for later use\n",
    "pickle.dump(words, open('words.pkl', 'wb'))  # Save words list as pickle file\n",
    "pickle.dump(classes, open('classes.pkl', 'wb'))  # Save classes list as pickle file\n",
    "# Training Data Preparation\n",
    "training = []  # List to store training data (bag-of-words + output vectors)\n",
    "outputEmpty = [0] * len(classes)  # Create an empty output vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996490bc-77b6-4cd1-9c3b-a1b0d8c29d74",
   "metadata": {},
   "source": [
    "### 3.Training Data Preparation:\n",
    "Creates a bag-of-words representation for each conversation pattern.\r\n",
    "This is a vector where each element represents a word in the vocabulary and its value indicates if the word appears in the pattern.\r\n",
    "Creates one-hot encoded output vectors for each intent class.\r\n",
    "These vectors have a length equal to the number of intent classes, with a 1 at the index corresponding to the intended class.\r\n",
    "Combines bag-of-words and output vectors for each pattern and intent.\r\n",
    "Shuffles the training data.\r\n",
    "Converts lists to NumPy arrays for efficient machine learning operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6a5224-7314-4d5c-87b6-612187f47372",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create bag-of-words representation and one-hot encoded output vector for each pattern-intent pair\n",
    "for document in documents:\n",
    "    bag = []  # List to store bag-of-words representation\n",
    "    wordPatterns = document[0]  # Get the word pattern from the document\n",
    "    wordPatterns = [lemmatizer.lemmatize(word.lower()) for word in wordPatterns]  # Lemmatize and lowercase words\n",
    "\n",
    "    # Check if each word in the vocabulary exists in the pattern\n",
    "    for word in words:\n",
    "        bag.append(1) if word in wordPatterns else bag.append(0)\n",
    "\n",
    "    outputRow = list(outputEmpty)  # Create a copy of the empty output vector\n",
    "    outputRow[classes.index(document[1])] = 1  # Set the element corresponding to the intent class to 1\n",
    "    training.append(bag + outputRow)  # Combine bag-of-words and output vector\n",
    "\n",
    "# Shuffle the training data for randomization\n",
    "random.shuffle(training)\n",
    "\n",
    "# Convert training data lists to NumPy arrays for efficient processing\n",
    "training = np.array(training)\n",
    "trainX = training[:, :len(words)]  # Separate input (bag-of-words) data\n",
    "trainY = training[:, len(words):]  # Separate output (one-hot encoded) data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef0698d-6e2f-484d-953b-709a08c38bc2",
   "metadata": {},
   "source": [
    "### 4.Model Building:\r",
    "Creates a sequential neural network model with TensorFlow Keras.\r\n",
    "The model has two hidden layers with ReLU activation and dropout for regularization.\r\n",
    "The output layer has a softmax activation for multi-class classification (one output for each intent class\n",
    ":\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a7058d-e28e-4a43-a3e6-225e96d56dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building\n",
    "model = tf.keras.Sequential()  # Create a sequential neural network model\n",
    "model.add(tf.keras.layers.Dense(128, input_shape=(len(trainX[0]),), activation='relu'))  # Hidden layer with 128 neurons, ReLU activation\n",
    "model.add(tf.keras.layers.Dropout(0.5))  # Dropout layer with 50% chance of dropping neurons for regularization\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))  # Second hidden layer with 64 neurons, ReLU activation\n",
    "model.add(tf.keras.layers.Dropout(0.5))  # Dropout layer with 50% chance of dropping neurons for regularization\n",
    "model.add(tf.keras.layers.Dense(len(trainY[0]), activation='softmax'))  # Output layer with softmax activation (one output for each intent class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19878b76-ac87-4afc-b164-8930563dfb75",
   "metadata": {},
   "source": [
    "### 5.Training:\n",
    "Defines an optimizer with specific learning rate and momentum for training the model.\r\n",
    "Compiles the model with categorical crossentropy loss for multi-class classification and accuracy metric.\r\n",
    "Trains the model on the prepared training data for 200 epochs with a batch size of 5.\r\n",
    "Saves the trained model as a .h5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abbc029-3f9e-4c3e-9f42-acddacc9017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(np.array(trainX), np.array(trainY), epochs=200, batch_size=5, verbose=1)\n",
    "model.save('chatbot_model.h5', hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e274355-1df6-4cdf-b01d-7f4d6bb749ed",
   "metadata": {},
   "source": [
    "### 6.Output:\n",
    "\r\n",
    "Prints a message indicating successful training completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95118d53-8787-4c68-939e-a4f760704f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f6864d-a2e9-43fa-9280-f7938652bab5",
   "metadata": {},
   "source": [
    "# 2.Chatbot Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e54d1ac-7d34-4c7f-8db5-e94f84069290",
   "metadata": {},
   "source": [
    "### Imports and Loading Data:\r\n",
    "\r\n",
    "Imports libraries for data manipulation, model loading, natural language processing, and time-related functions.\r\n",
    "Loads a WordNetLemmatizer for word normalization.\r\n",
    "Loads chatbot intents from a JSON file.\r\n",
    "Loads the vocabulary and classes from pickle files created during model training.\r\n",
    "Loads the trained chatbot model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a628ed9e-9249-49f5-b1d0-b00678a3ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from datetime import datetime\n",
    "import wikipedia\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.models import load_model\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "intents = json.loads(open('D:\\Python Projects\\ChatBot\\Intent.json').read())\n",
    "\n",
    "words = pickle.load(open('words.pkl', 'rb'))\n",
    "classes = pickle.load(open('classes.pkl', 'rb'))\n",
    "model = load_model('chatbot_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a5dc8b-6e70-431f-9717-d95037d9870c",
   "metadata": {},
   "source": [
    "### Text Preprocessing Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813df6f1-04bc-4f0d-9028-d43a109157ea",
   "metadata": {},
   "source": [
    "#### clean_up_sentence(sentence):\r\n",
    "\r\n",
    "Tokenizes a sentence into words using nltk.word_tokenize.\r\n",
    "Lemmatizes each word using the WordNetLemmatizer (reduces to base form).\r\n",
    "Returns a list of cleaned words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a765d5a-07e4-4f74-bfdf-057f363088e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]\n",
    "    return sentence_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7798609-af47-4284-ad0b-4bb5eff5c638",
   "metadata": {},
   "source": [
    "#### bag_of_words(sentence):\r\n",
    "\r\n",
    "Cleans the sentence using clean_up_sentence.\r\n",
    "Creates a bag-of-words representation: a vector where each element indicates the presence or absence of a word from the vocabulary.\r\n",
    "Returns a NumPy array representing the bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf485e4-08b0-4d4b-905e-a4b148e97942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words (sentence):\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    bag = [0] * len(words)\n",
    "    for w in sentence_words:\n",
    "        for i, word in enumerate(words):\n",
    "            if word == w:\n",
    "                bag[i] = 1\n",
    "    return np.array(bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec54becd-a69a-4b2f-a570-df7748eb1bd0",
   "metadata": {},
   "source": [
    "### Intent Prediction:\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515a8d9b-ad60-4253-825b-c9d04ca73994",
   "metadata": {},
   "source": [
    "#### predict_class(sentence):\r\n",
    "Converts the sentence to a bag-of-words using bag_of_words.\r\n",
    "Passes the bag-of-words to the model for prediction.\r\n",
    "Filters results above a threshold (0.25) for higher confidence.\r\n",
    "Sorts results by probability in descending order.\r\n",
    "Returns a list of predicted intents with scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5673f7-f492-430a-970b-4e5cf64918c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class (sentence):\n",
    "    bow = bag_of_words (sentence)\n",
    "    res = model.predict(np.array([bow]))[0]\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
    "\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({'intent': classes [r[0]], 'probability': str(r[1])})\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a929d04-00bf-4027-a1bb-a0bf47f78f9c",
   "metadata": {},
   "source": [
    "### Response Generation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f67ba3-d54f-49dc-8229-2f0da6b16c9c",
   "metadata": {},
   "source": [
    "#### get_response(intents_list, intents_json):\r\n",
    "Takes a list of predicted intents and the loaded intents JSON.\r\n",
    "Finds a matching intent in the JSON and randomly selects a response from it.\r\n",
    "If an extension response is available, randomly selects one.\r\n",
    "Returns the chosen response and extension (or empty string if no extension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40554c63-cd6f-4594-ba81-9e467afa0967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(intents_list, intents_json):\n",
    "  intent = intents_list[0]['intent']\n",
    "  list_of_intents = intents_json['intents']\n",
    "  for i in list_of_intents:\n",
    "    if i['intent'] == intent:\n",
    "      result = random.choice(i['responses'])\n",
    "      extension = random.choice(i['extension']['responses']) if len(i['extension']['responses']) > 0 else \"\"  # Empty string if no extension response\n",
    "      return result, extension\n",
    "  return result, \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7647a08f-d503-4813-a7cf-8add9f648914",
   "metadata": {},
   "source": [
    "### Main Interaction Loop:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139aa220-36b0-45bc-ab01-440809ba1c6b",
   "metadata": {},
   "source": [
    "#### Prints a message indicating the chatbot is running.\r",
    "#### \n",
    "Enters an infinite loop:\r\n",
    "Prompts the user for input.\r\n",
    "Predicts the intent of the user's message using predict_class.\r\n",
    "Generates a response using get_response.\r\n",
    "Prints the generated response to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3313c0-8105-4507-8951-f24ebf507103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GO! Bot is running!\n"
     ]
    }
   ],
   "source": [
    "print(\"GO! Bot is running!\")\n",
    "\n",
    "while True:\n",
    "    message = input(\"\")\n",
    "    ints = predict_class (message)\n",
    "    res = get_response (ints, intents)\n",
    "    print (res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
